# -*- coding: utf-8 -*-
"""Image Captioning Using Conditioned LSTM RNN Generators.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hEOAAkE7uzWa7phRX_qktymPPGki6HPM
"""

# Commented out IPython magic to ensure Python compatibility.
import os
from collections import defaultdict
import numpy as np
import PIL
from matplotlib import pyplot as plt
# %matplotlib inline

from tensorflow.keras import Sequential, Model
from tensorflow.keras.layers import Embedding, LSTM, Dense, Input, Bidirectional, RepeatVector, Concatenate, Activation
from tensorflow.keras.activations import softmax
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.preprocessing.sequence import pad_sequences

from tensorflow.keras.applications.inception_v3 import InceptionV3

from tensorflow.keras.optimizers import Adam

! wget https://storage.googleapis.com/4705-hw5-data/hw5data-20220809T182644Z-001.zip

! unzip hw5data-20220809T182644Z-001.zip

FLICKR_PATH="hw5data"

def load_image_list(filename):
    with open(filename,'r') as image_list_f:
        return [line.strip() for line in image_list_f]

train_list = load_image_list(os.path.join(FLICKR_PATH, 'Flickr_8k.trainImages.txt'))
dev_list = load_image_list(os.path.join(FLICKR_PATH,'Flickr_8k.devImages.txt'))
test_list = load_image_list(os.path.join(FLICKR_PATH,'Flickr_8k.testImages.txt'))

len(train_list), len(dev_list), len(test_list)

dev_list[20]

IMG_PATH = os.path.join(FLICKR_PATH, "Flickr8k_Dataset")

image = PIL.Image.open(os.path.join(IMG_PATH, dev_list[20]))
image

plt.imshow(image)

np.asarray(image).shape

np.asarray(image)

new_image = np.asarray(image.resize((299,299))) / 255.0
plt.imshow(new_image)

new_image.shape

def get_image(image_name):
    image = PIL.Image.open(os.path.join(IMG_PATH, image_name))
    return np.asarray(image.resize((299,299))) / 255.0

plt.imshow(get_image(dev_list[25]))

img_model = InceptionV3(weights='imagenet')

img_model.summary() # this is quite a complex model.

new_input = img_model.input
new_output = img_model.layers[-2].output
img_encoder = Model(new_input, new_output)

encoded_image = img_encoder.predict(np.array([new_image]))

encoded_image

def img_generator(img_list):
    ret = np.zeros((1,299,299,3))
    for img in img_list:
        image = PIL.Image.open(os.path.join(IMG_PATH, img))
        new_image = np.asarray(image.resize((299,299))) / 255.0
        ret[0] = new_image
        yield ret

enc_train = img_encoder.predict_generator(img_generator(train_list), steps=len(train_list), verbose=1)

enc_train[11]

enc_dev = img_encoder.predict_generator(img_generator(dev_list), steps=len(dev_list), verbose=1)

enc_test = img_encoder.predict_generator(img_generator(test_list), steps=len(test_list), verbose=1)

# Choose a suitable location here, please do NOT attempt to write your output files to the shared data directory.
OUTPUT_PATH = "hw5output"
if not os.path.exists(OUTPUT_PATH):
    os.mkdir(OUTPUT_PATH)

np.save(os.path.join(OUTPUT_PATH,"encoded_images_train.npy"), enc_train)
np.save(os.path.join(OUTPUT_PATH,"encoded_images_dev.npy"), enc_dev)
np.save(os.path.join(OUTPUT_PATH,"encoded_images_test.npy"), enc_test)

def read_image_descriptions(filename):
    image_descriptions = defaultdict(list)
    f = open(filename, 'r')
    lines = f.readlines()
    count = 0
    captions = []
    for l in lines:
        l = l[0:len(l)-1]
        tokens = l.split('#')
        img_name = tokens[0]
        caption = ['<START>']
        cap = tokens[1].split('\t')[1].lower()
        caption += cap.split(' ')
        caption += ['<END>']
        captions.append(caption)
        count += 1
        if count == 5:
            image_descriptions[img_name] = captions
            count = 0
            captions = []
    f.close()
    return image_descriptions

descriptions = read_image_descriptions(f"{FLICKR_PATH}/Flickr8k.token.txt")

print(descriptions[dev_list[0]])

tokens = set()
for cList in descriptions.values():
    for caption in cList:
        for tok in caption:
            tokens.add(tok)
tokens = sorted(list(set(tokens)))
id_to_word = {v: k for v, k in enumerate(tokens)}

word_to_id = {k: v for v, k in enumerate(tokens)}

word_to_id['dog']

id_to_word[1985]

max(len(description) for image_id in train_list for description in descriptions[image_id])

MAX_LEN = 40
EMBEDDING_DIM=300
vocab_size = len(word_to_id)

# Text input
text_input = Input(shape=(MAX_LEN,))
embedding = Embedding(vocab_size, EMBEDDING_DIM, input_length=MAX_LEN)(text_input)
x = Bidirectional(LSTM(512, return_sequences=False))(embedding)
pred = Dense(vocab_size, activation='softmax')(x)
model = Model(inputs=[text_input],outputs=pred)
model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])

model.summary()

def text_training_generator(batch_size=128):
    i = 0
    inputs = np.zeros((batch_size, MAX_LEN))
    outputs = np.zeros((batch_size, vocab_size))
    while True:
        for img_id in train_list:
            for description in descriptions[img_id]:
                tokenized = [word_to_id[word] for word in description]
                for j in range(1, len(description)):
                    inRow = np.zeros(MAX_LEN)
                    outRow = np.zeros(vocab_size)
                    inRow[:j] = tokenized[:j]
                    nxt = tokenized[j]
                    outRow[nxt] = 1
                    inputs[i] = inRow
                    outputs[i] = outRow
                    i += 1
                    if i%(batch_size)==0:
                        yield((inputs, outputs))
                        i = 0
                        inp = np.zeros((batch_size, MAX_LEN))
                        out = np.zeros((batch_size, vocab_size))

batch_size = 128
generator = text_training_generator(batch_size)
steps = len(train_list) * MAX_LEN // batch_size

model.fit_generator(generator, steps_per_epoch=steps, verbose=True, epochs=10)

def decoder():
    input = np.zeros((1,MAX_LEN))
    input[0][0] = word_to_id['<START>']
    for i in range(1, MAX_LEN):
      output = model.predict(input)[0]
      id = output.argmax()
      input[0][i] = id
      if id == word_to_id['<END>']:
        return [id_to_word[id] for id in input[0]]
    return [id_to_word[id] for id in input[0]]

print(decoder())

def sample_decoder():
    input = np.zeros((1,MAX_LEN))
    input[0][0] = word_to_id['<START>']
    for i in range(1, MAX_LEN):
      out = model.predict(input)[0]
      out = np.asarray(out).astype('float64')
      out = out / np.sum(out)
      prob = np.random.multinomial(1, out, 1)
      id = prob.argmax()
      input[0][i] = id
      if id == word_to_id['<END>']:
        return [id_to_word[id] for id in input[0]]
    return [id_to_word[id] for id in input[0]]

for i in range(10):
    print(sample_decoder())

MAX_LEN = 40
EMBEDDING_DIM=300
IMAGE_ENC_DIM=300

# Image input
img_input = Input(shape=(2048,))
img_enc = Dense(300, activation="relu") (img_input)
images = RepeatVector(MAX_LEN)(img_enc)

# Text input
text_input = Input(shape=(MAX_LEN,))
embedding = Embedding(vocab_size, EMBEDDING_DIM, input_length=MAX_LEN)(text_input)
x = Concatenate()([images,embedding])
y = Bidirectional(LSTM(256, return_sequences=False))(x)
pred = Dense(vocab_size, activation='softmax')(y)
model = Model(inputs=[img_input,text_input],outputs=pred)
model.compile(loss='categorical_crossentropy', optimizer="RMSProp", metrics=['accuracy'])

enc_train = np.load(f"{OUTPUT_PATH}/encoded_images_train.npy")
enc_dev = np.load(f"{OUTPUT_PATH}/encoded_images_dev.npy")

def training_generator(batch_size=128):
    i = 0
    imgIn = np.zeros((batch_size, 2048))
    textIn = np.zeros((batch_size, MAX_LEN))
    outputs = np.zeros((batch_size, vocab_size))
    while True:
        img_idx = 0
        for img_id in train_list:
            img = get_image(img_id)
            for description in descriptions[img_id]:
                tokenized = [word_to_id[word] for word in description]
                for j in range(1, len(description)):
                    inRow = np.zeros(MAX_LEN)
                    outRow = np.zeros(vocab_size)
                    inRow[:j] = tokenized[:j]
                    nxt = tokenized[j]
                    outRow[nxt] = 1
                    imgIn[i] = enc_train[img_idx]
                    textIn[i] = inRow
                    outputs[i] = outRow
                    i += 1
                    if i%(batch_size)==0:
                        yield(([imgIn, textIn], outputs))
                        i = 0
                        inp = np.zeros((batch_size, MAX_LEN))
                        out = np.zeros((batch_size, vocab_size))
            img_idx += 1

batch_size = 128
generator = training_generator(batch_size)
steps = len(train_list) * MAX_LEN // batch_size

model.fit_generator(generator, steps_per_epoch=steps, verbose=True, epochs=20)

def image_decoder(enc_image):
    img = np.zeros((1, 2048))
    input = np.zeros((1,MAX_LEN))
    input[0][0] = word_to_id['<START>']
    img[0] = enc_image
    for i in range(1, MAX_LEN):
      out = model.predict([img, input])[0]
      out = np.asarray(out).astype('float64')
      out = out / np.sum(out)
      prob = np.random.multinomial(1, out, 1)
      id = prob.argmax()
      input[0][i] = id
      if id == word_to_id['<END>']:
        return [id_to_word[id] for id in input[0]]
    return [id_to_word[id] for id in input[0]]

plt.imshow(get_image(train_list[0]))
image_decoder(enc_train[0])

plt.imshow(get_image(dev_list[1]))
image_decoder(enc_dev[1])

import keras
keras.utils.disable_interactive_logging()

def image_decoder(enc_image):
    img = np.zeros((1, 2048))
    input = np.zeros((1,MAX_LEN))
    input[0][0] = word_to_id['<START>']
    img[0] = enc_image
    beam = [(1, input)]
    for s in beam:
      for i in range(1, MAX_LEN):
        out = model.predict([img, s[1]])[0]
        out = np.asarray(out).astype('float64')
        out = out / np.sum(out)
        prob = np.random.multinomial(1, out, 1)
        id = prob.argmax()

        s[1][0][i] = id
        if id == word_to_id['<END>']:
          return [id_to_word[id] for id in input[0]]
      # prune
    return [id_to_word[id] for id in input[0]]

def beam_decoder(beam_width, enc_image):
    keras.utils.disable_interactive_logging()
    img = np.zeros((1, 2048))
    input = np.zeros((1, MAX_LEN))
    input[0][0] = word_to_id['<START>']
    img[0] = enc_image

    beam = [(0.0, input)]
    for i in range(1, MAX_LEN):
        candidates = []
        for prob, seq in beam:
            out = model.predict([img, seq])[0]
            out = np.asarray(out).astype('float64')
            out = out / np.sum(out)
            word_ids = np.argsort(out)[-beam_width:]  # Select top beam_width words
            for word_id in word_ids:
                candidate_seq = seq.copy()
                candidate_seq[0][i] = word_id
                candidates.append((prob + np.log(out[word_id]), candidate_seq))

        candidates.sort(reverse=True, key=lambda x: x[0])
        beam = candidates[:beam_width]

    best_seq = beam[0][1]
    caption = []
    for word_id in best_seq[0]:
        caption.append(id_to_word[word_id])
        if id_to_word[word_id] == '<END>':
            break

    return caption

"""5 development images, each with 1) their greedy output, 2) beam search at n=3"""

plt.imshow(get_image(dev_list[1]))

image_decoder(enc_dev[1])

beam_decoder(3, enc_dev[1])

plt.imshow(get_image(dev_list[3]))

image_decoder(enc_dev[3])

beam_decoder(3, enc_dev[3])

plt.imshow(get_image(dev_list[104]))

image_decoder(enc_dev[104])

beam_decoder(3, enc_dev[104])

plt.imshow(get_image(dev_list[12]))

image_decoder(enc_dev[12])

beam_decoder(3, enc_dev[12])

plt.imshow(get_image(dev_list[18]))

image_decoder(enc_dev[18])

beam_decoder(3, enc_dev[18])

"""~"""